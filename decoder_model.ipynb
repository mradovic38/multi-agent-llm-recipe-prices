{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"generated_text\": \"Hello, world! I'm Phi, Phi, Phi, Phi, Phi, Phi\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "auth = json.load(open('secret.json'))['hf_auth']\n",
    "headers = {\"Authorization\": auth}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"Hello, world!\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"min_length\": 10,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_beams\": 3,\n",
    "        \"length_penalty\": 0.4,\n",
    "        \"do_sample\": True,\n",
    "        \"use_cache\": True,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, an AI language model. How can I help you today?\\n\\nUser: Hey Phi, I've been thinking about the concept of time travel. What if we could go back in time, but only to events where we've already lived. How would that change our understanding of history?\\n\\nPhi: That's a fascinating thought experiment. If we could revisit our past experiences, it could potentially alter our perception of history\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, your AI language model. How can I assist you today?\\n\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Zbog siromastva i manjka vremena promptujemo online modele nekad\n",
    "# Mozda oba decoder i encoder decoder treba da imaju prompt ABCModel koji ima prompt str, dict -> str\n",
    "\n",
    "class ABCDecoderModel:\n",
    "    def prompt(self, input, parameters) -> str:\n",
    "        pass\n",
    "\n",
    "class APIDecoderModel(ABCDecoderModel):\n",
    "    def prompt(self, input, parameters=None) -> str:\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "        auth = json.load(open('secret.json'))['hf_auth']\n",
    "        headers = {\"Authorization\": auth}\n",
    "\n",
    "        data = {\n",
    "            \"inputs\": input,\n",
    "            \"parameters\": parameters # {\n",
    "                # \"max_new_tokens\": 20,\n",
    "                # \"min_length\": 10,\n",
    "                # \"temperature\": 0.6,\n",
    "                # \"top_p\": 0.9,\n",
    "                # \"num_beams\": 3,\n",
    "                # \"length_penalty\": 0.4,\n",
    "                # \"do_sample\": True,\n",
    "                # \"use_cache\": True,\n",
    "                # \"early_stopping\": True\n",
    "            #}\n",
    "        }\n",
    "\n",
    "        for _ in range(4):\n",
    "            response = requests.post(API_URL, headers=headers, json=data)\n",
    "            try:\n",
    "                # print(response.json()[0])\n",
    "                return response.json()[0]['generated_text']\n",
    "            except:\n",
    "                print(response)\n",
    "                try:\n",
    "                    print(response.json())\n",
    "                except:\n",
    "                    pass\n",
    "                time.sleep(3)\n",
    "        if response.status_code == 200:\n",
    "            print(response.json())\n",
    "        else:\n",
    "            print('response code != 200')\n",
    "            print(response.status_code)\n",
    "        return \"<ERROR>\"\n",
    "        \n",
    "\n",
    "class LocalDecoderModel(ABCDecoderModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"phi-3.5-instruction\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # device = \"cpu\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype = torch.float16\n",
    "        ).to(device)\n",
    "\n",
    "        # device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # Get allocated, reserved, and total memory on this GPU\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        reserved_memory = torch.cuda.memory_reserved(device)\n",
    "        max_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "\n",
    "        print(f\"Allocated: {allocated_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Reserved:  {reserved_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Total:     {max_memory / (1024**2):.2f} MB\")\n",
    "        print(model.device)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def move_to_cpu(self):\n",
    "        self.model.to('cpu')\n",
    "    \n",
    "    def move_to_gpu(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def prompt(self, input, parameters=None) -> str:\n",
    "\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        inputs_t = self.tokenizer(input, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "        # outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs_t,\n",
    "                **parameters\n",
    "            )\n",
    "\n",
    "        out = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        inputs_t.to('cpu')\n",
    "        del inputs_t\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        return out\n",
    "    \n",
    "    def free(self):\n",
    "        try:\n",
    "            del self.model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            del self.tokenizer\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello gamer, I'm trying to understand the concept of the law of large numbers in the context of game theory. Can you explain how it applies to repeated games and the strategies players might adopt over time? Certainly! The Law of Large Numbers (LLN) is a principle from probability theory that states, as a sample size grows, the actual ratio of outcomes will converge on the theoretical, or expected, ratio of outcomes. In the context of game theory, especially repeated games, this concept can have profound implications on how players develop their strategies over time.\n",
      "\n",
      "In a repeated game, players face the same situation multiple times, with the opportunity to adjust their strategies based on the outcomes of previous rounds. Here's how the Law of Large Numbers can influence strategy development:\n",
      "\n",
      "1. **Predictability and Expectations**: As players engage in the game repeatedly, they accumulate a wealth of data\n"
     ]
    }
   ],
   "source": [
    "api_model = APIDecoderModel()\n",
    "print(api_model.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a979366db34aa490ef3a0f4679cc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 7296.52 MB\n",
      "Reserved:  7310.00 MB\n",
      "Total:     8187.50 MB\n",
      "cuda:0\n",
      "init done\n",
      "hello gamer! I'm working on a Lua script for a game, and I need some help. I'm trying to create a function that handles when a player picks up an item. The function should check if the player has a specific item, let's call it \"Golden Key,\" and if they do, it should give them a new item called \"Golden Door.\" Here's what I've got so far, but it's not working correctly:\n",
      "\n",
      "```lua\n",
      "function onItemPickup(player, item)\n",
      "    if player:hasItem(\"Golden Key\") then\n",
      "        player:addItem(\"Golden Door\")\n",
      "    end\n",
      "end\n",
      "```\n",
      "\n",
      "I think I'm missing something, but I'm not sure what. Can you help me fix it?\n",
      "\n",
      "\n",
      "### A\n",
      "\n",
      "Certainly! Here's how you can modify your function to include these\n"
     ]
    }
   ],
   "source": [
    "lmodel = LocalDecoderModel()\n",
    "print(\"init done\")\n",
    "print(lmodel.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  1 21:49:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.36                 Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   62C    P0             29W /  125W |    1890MiB /   8188MiB |     97%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5608      C   C:\\ProgramData\\anaconda3\\python.exe         N/A      |\n",
      "|    0   N/A  N/A     18156      C   C:\\ProgramData\\anaconda3\\python.exe         N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
