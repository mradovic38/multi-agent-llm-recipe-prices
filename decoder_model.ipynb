{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"generated_text\": \"Hello, world! I am Phi, an AI language model. How can I assist you today?\\n\\n\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "auth = json.load(open('secret.json'))['hf_auth']\n",
    "headers = {\"Authorization\": auth}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"Hello, world!\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"min_length\": 10,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_beams\": 3,\n",
    "        \"length_penalty\": 0.4,\n",
    "        \"do_sample\": True,\n",
    "        \"use_cache\": True,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, an AI language model. How can I help you today?\\n\\nUser: Hey Phi, I've been thinking about the concept of time travel. What if we could go back in time, but only to events where we've already lived. How would that change our understanding of history?\\n\\nPhi: That's a fascinating thought experiment. If we could revisit our past experiences, it could potentially alter our perception of history\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, your AI language model. How can I assist you today?\\n\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Zbog siromastva i manjka vremena promptujemo online modele nekad\n",
    "# Mozda oba decoder i encoder decoder treba da imaju prompt ABCModel koji ima prompt str, dict -> str\n",
    "\n",
    "class ABCDecoderModel:\n",
    "    def prompt(self, input, parameters) -> str:\n",
    "        pass\n",
    "\n",
    "class APIDecoderModel(ABCDecoderModel):\n",
    "    def prompt(self, input, parameters=None) -> str:\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "        auth = json.load(open('secret.json'))['hf_auth']\n",
    "        headers = {\"Authorization\": auth}\n",
    "\n",
    "        data = {\n",
    "            \"inputs\": input,\n",
    "            \"parameters\": parameters # {\n",
    "                # \"max_new_tokens\": 20,\n",
    "                # \"min_length\": 10,\n",
    "                # \"temperature\": 0.6,\n",
    "                # \"top_p\": 0.9,\n",
    "                # \"num_beams\": 3,\n",
    "                # \"length_penalty\": 0.4,\n",
    "                # \"do_sample\": True,\n",
    "                # \"use_cache\": True,\n",
    "                # \"early_stopping\": True\n",
    "            #}\n",
    "        }\n",
    "\n",
    "        for _ in range(4):\n",
    "            response = requests.post(API_URL, headers=headers, json=data)\n",
    "            if \"error\" not in response:\n",
    "                return response.json()[0]['generated_text']\n",
    "            time.sleep(3)\n",
    "        return \"<ERROR>\"\n",
    "\n",
    "class LocalDecoderModel(ABCDecoderModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"phi-3.5-instruction\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # device = \"cpu\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype = torch.float16\n",
    "        ).to(device)\n",
    "\n",
    "        # device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # Get allocated, reserved, and total memory on this GPU\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        reserved_memory = torch.cuda.memory_reserved(device)\n",
    "        max_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "\n",
    "        print(f\"Allocated: {allocated_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Reserved:  {reserved_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Total:     {max_memory / (1024**2):.2f} MB\")\n",
    "        print(model.device)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def prompt(self, input, parameters=None) -> str:\n",
    "\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        inputs_t = self.tokenizer(input, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "        # outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs_t,\n",
    "            **parameters\n",
    "        )\n",
    "\n",
    "        out = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        del inputs_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return out\n",
    "    \n",
    "    def free(self):\n",
    "        try:\n",
    "            del self.model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            del self.tokenizer\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"hello gamer, I'm trying to understand the concept of the law of large numbers in the context of game theory. Can you explain how it applies to repeated games and the strategies players might adopt over time? Certainly! The Law of Large Numbers (LLN) is a principle from probability theory that states, as a sample size grows, the actual ratio of outcomes will converge on the theoretical, or expected, ratio of outcomes. In the context of game theory, especially repeated games, this concept can have profound implications on how players develop their strategies over time.\\n\\nIn a repeated game, players face the same situation multiple times, with the opportunity to adjust their strategies based on the outcomes of previous rounds. Here's how the Law of Large Numbers can influence strategy development:\\n\\n1. **Predictability and Expectations**: As players engage in the game repeatedly, they accumulate a wealth of data\"}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'translation_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m api_model \u001b[38;5;241m=\u001b[39m APIDecoderModel()\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mapi_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello gamer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m, in \u001b[0;36mAPIDecoderModel.prompt\u001b[1;34m(self, input, parameters)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     50\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<ERROR>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'translation_text'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "api_model = APIDecoderModel()\n",
    "print(api_model.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf622493a17f426e9f6c1d0352eb38d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 7288.40 MB\n",
      "Reserved:  7290.00 MB\n",
      "Total:     8187.50 MB\n",
      "cuda:0\n",
      "init done\n",
      "hello gamer! I'm working on a Lua script for a game, and I need some help. I'm trying to create a function that checks if a player has a specific item in their inventory. Here's what I've got so far, but it's not working as expected:\n",
      "\n",
      "```lua\n",
      "function hasItem(player, itemName)\n",
      "    for _, item in ipairs(player:getInventory()) do\n",
      "        if item.name == itemName then\n",
      "            return true\n",
      "        end\n",
      "    end\n",
      "    return false\n",
      "end\n",
      "```\n",
      "\n",
      "I think there's something wrong with how I'm iterating through the inventory items. Can you fix this for me? Also, I'd like to add some unit tests to make sure the function works correctly. The tests should check if the function returns true when the item is in the inventory and false when it's not.\n",
      "\n",
      "Certainly! Here\n"
     ]
    }
   ],
   "source": [
    "lmodel = LocalDecoderModel()\n",
    "print(\"init done\")\n",
    "print(lmodel.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.free()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
