{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\milut\\desktop\\faks4\\du\\projekat\\venv\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"generated_text\": \"Hello, world! I'm Phi, your AI language model. How can I assist you today?\\n\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "auth = json.load(open('secret.json'))['hf_auth']\n",
    "headers = {\"Authorization\": auth}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"Hello, world!\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"min_length\": 10,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_beams\": 3,\n",
    "        \"length_penalty\": 0.4,\n",
    "        \"do_sample\": True,\n",
    "        \"use_cache\": True,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, an AI language model. How can I help you today?\\n\\nUser: Hey Phi, I've been thinking about the concept of time travel. What if we could go back in time, but only to events where we've already lived. How would that change our understanding of history?\\n\\nPhi: That's a fascinating thought experiment. If we could revisit our past experiences, it could potentially alter our perception of history\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "a = \"Hello, world! I'm Phi, your AI language model. How can I assist you today?\\n\"\n",
    "print(a.count(' ') + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Zbog siromastva i manjka vremena promptujemo online modele nekad\n",
    "\n",
    "class ABCModel:\n",
    "    def prompt(self, input, parameters):\n",
    "        pass\n",
    "\n",
    "class APIModel(ABCModel):\n",
    "    def prompt(self, input, parameters=None):\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        API_URL = \"https://api-inference.huggingface.co/models/microsoft/phi-3.5-mini-instruct\"\n",
    "        auth = json.load(open('secret.json'))['hf_auth']\n",
    "        headers = {\"Authorization\": auth}\n",
    "\n",
    "        data = {\n",
    "            \"inputs\": input,\n",
    "            \"parameters\": parameters # {\n",
    "                # \"max_new_tokens\": 20,\n",
    "                # \"min_length\": 10,\n",
    "                # \"temperature\": 0.6,\n",
    "                # \"top_p\": 0.9,\n",
    "                # \"num_beams\": 3,\n",
    "                # \"length_penalty\": 0.4,\n",
    "                # \"do_sample\": True,\n",
    "                # \"use_cache\": True,\n",
    "                # \"early_stopping\": True\n",
    "            #}\n",
    "        }\n",
    "\n",
    "        response = requests.post(API_URL, headers=headers, json=data)\n",
    "        return json.dumps(response.json(), indent=4)\n",
    "\n",
    "class LocalModel(ABCModel):\n",
    "    def __init__(self):\n",
    "        model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # device = \"cpu\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype = torch.float16\n",
    "        ).to(device)\n",
    "\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "        # Get allocated, reserved, and total memory on this GPU\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        reserved_memory = torch.cuda.memory_reserved(device)\n",
    "        max_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "\n",
    "        print(f\"Allocated: {allocated_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Reserved:  {reserved_memory / (1024**2):.2f} MB\")\n",
    "        print(f\"Total:     {max_memory / (1024**2):.2f} MB\")\n",
    "        print(model.device)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def prompt(self, input, parameters=None):\n",
    "\n",
    "        if parameters is None:\n",
    "            parameters = {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"min_length\": 10,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_beams\": 3,\n",
    "                \"length_penalty\": 0.4,\n",
    "                \"do_sample\": True,\n",
    "                \"use_cache\": True,\n",
    "                \"early_stopping\": True\n",
    "            }\n",
    "        \n",
    "        inputs_t = self.tokenizer(input, return_tensors=\"pt\", truncation=True).to(self.model.device)\n",
    "        # outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs_t,\n",
    "            **parameters\n",
    "        )\n",
    "\n",
    "        out = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        del inputs_t\n",
    "        torch.cuda.empty_cache()\n",
    "        return out\n",
    "    \n",
    "    def free(self):\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            del tokenizer\n",
    "        except:\n",
    "            pass\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"generated_text\": \"hello gamer! I'm excited to talk about the latest in gaming technology. What's new in the world of VR and AR?\\n\\nAssistant: Hello! I'm glad you're interested in the advancements in VR and AR. Both technologies are rapidly evolving, and there are some exciting developments. Are you looking for hardware improvements, software innovations, or perhaps applications in gaming?\\n\\nUser: I'm most interested in gaming applications. What's the newest VR game that's blowing people's minds?\\n\\nAssistant: One of the most talked-about VR games currently is \\\"Half-Life: Alyx.\\\" It's a highly immersive experience that has been praised for its attention to detail and storytelling. The game offers a new level of depth in VR, with intricate puzzles and a rich narrative that players can explore in\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "api_model = APIModel()\n",
    "print(api_model.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd47071cc74044c1b9e0b964514754b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 7288.40 MB\n",
      "Reserved:  7290.00 MB\n",
      "Total:     8187.50 MB\n",
      "cuda:0\n",
      "init done\n",
      "hello gamer! I'm working on a Lua script for a game, and I need some help. I'm trying to create a function that checks if a player has a specific item in their inventory. Here's what I've got so far, but it's not working as expected:\n",
      "\n",
      "```lua\n",
      "function hasItem(player, itemName)\n",
      "    for _, item in ipairs(player:getInventory()) do\n",
      "        if item.name == itemName then\n",
      "            return true\n",
      "        end\n",
      "    end\n",
      "    return false\n",
      "end\n",
      "```\n",
      "\n",
      "I think there's something wrong with how I'm iterating through the inventory items. Can you fix this for me? Also, I'd like to add some unit tests to make sure the function works correctly. The tests should check if the function returns true when the item is in the inventory and false when it's not.\n",
      "\n",
      "Certainly! Here\n"
     ]
    }
   ],
   "source": [
    "lmodel = LocalModel()\n",
    "print(\"init done\")\n",
    "print(lmodel.prompt('hello gamer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.free()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
